{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PH2 Dataset",
      "provenance": [],
      "authorship_tag": "ABX9TyOmePKd+qt2wllir/3oFr2x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hugosousa111/ph2_dataset_lesion_features/blob/main/PH2_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fii40J-AqBkN",
        "outputId": "1dd7f093-214b-4075-cd24-60913905e4a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Acessa o Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhdmzW1voyr0"
      },
      "source": [
        "# Bibliotecas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import random\n",
        "random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmJ7zB9YpB04"
      },
      "source": [
        "# Pegando a base\n",
        "df = pd.read_excel('/content/drive/My Drive/PH2Dataset/PH2_dataset.xlsx')\n",
        "df.columns = [\"Image Name\",\"Histological Diagnosis\",\"Common Nevus\",\"Atypical Nevus\", \"Melanoma\", \"Asymmetry(0/1/2)\", \"Pigment Network(AT/T)\", \"Dots/Globules(A/AT/T)\", \"Streaks\\n(A/P)\",\"Regression Areas(A/P)\", \"Blue-Whitish Veil(A/P)\", \"White\", \"Red\", \"Light-Brown\", \"Dark-Brown\", \"Blue-Gray\", \"Black\"]\n",
        "\n",
        "# Tratando a legenda\n",
        "legenda = df.iloc[1:8,]\n",
        "legenda = legenda[['Pigment Network(AT/T)','Dots/Globules(A/AT/T)']]\n",
        "legenda.columns = ['Sigla', 'Significado']\n",
        "legenda.reset_index\n",
        "\n",
        "# Apenas os dados\n",
        "df = df.iloc[12:,] #apagando legenda\n",
        "df = df.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32At6jaIuk_3"
      },
      "source": [
        "#df\n",
        "#legenda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47KYrHOquk3v"
      },
      "source": [
        "# Função que pega o rótulo de uma amostra dada o seu nome\n",
        "def get_rotulo(img, df):\n",
        "    if df[\"Common Nevus\"][df[\"Image Name\"] == img].values == 'X':\n",
        "        return 0\n",
        "    elif df[\"Atypical Nevus\"][df[\"Image Name\"] == img].values  == 'X':\n",
        "        return 1\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "# Coloco os rótulos em 0, 1 e 2\n",
        "Images_Name_column = df['Image Name']\n",
        "rotulos = np.zeros(200)\n",
        "cont = 0\n",
        "for i in Images_Name_column: \n",
        "    rotulos[cont] = get_rotulo(i, df)\n",
        "    cont +=1\n",
        "\n",
        "# Coloca esse vetor de rotulos em uma das colunas\n",
        "df['Common Nevus'] = rotulos.astype(int)\n",
        "df=df.rename(columns = {'Common Nevus':'Label'})\n",
        "df\n",
        "\n",
        "# Apaga as outras colunas\n",
        "df = df.drop(columns=['Atypical Nevus','Melanoma'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wpHcsPLzhoa"
      },
      "source": [
        "# Procurando em cada coluna se tem NaN\n",
        "ver = 0\n",
        "if ver == 1: \n",
        "    print('Image Name')\n",
        "    print(df['Image Name'].isnull().sum())\n",
        "    print('Histological Diagnosis')\n",
        "    print(df['Histological Diagnosis'].isnull().sum())\n",
        "    print(\"Asymmetry(0/1/2)\")\n",
        "    print(df[\"Asymmetry(0/1/2)\"].isnull().sum())\n",
        "    print(\"Pigment Network(AT/T)\")\n",
        "    print(df[\"Pigment Network(AT/T)\"].isnull().sum())\n",
        "    print(\"Dots/Globules(A/AT/T)\")\n",
        "    print(df[\"Dots/Globules(A/AT/T)\"].isnull().sum())\n",
        "    print(\"Streaks\\n(A/P)\")\n",
        "    print(df[\"Streaks\\n(A/P)\"].isnull().sum())\n",
        "    print(\"Regression Areas(A/P)\")\n",
        "    print(df[\"Regression Areas(A/P)\"].isnull().sum())\n",
        "    print(\"Blue-Whitish Veil(A/P)\")\n",
        "    print(df[\"Blue-Whitish Veil(A/P)\"].isnull().sum())\n",
        "    print(\"White\")\n",
        "    print(df[\"White\"].isnull().sum())\n",
        "    print(\"Red\")\n",
        "    print(df[\"Red\"].isnull().sum())\n",
        "    print(\"Light-Brown\")\n",
        "    print(df[\"Light-Brown\"].isnull().sum())\n",
        "    print(\"Dark-Brown\")\n",
        "    print(df[\"Dark-Brown\"].isnull().sum())\n",
        "    print(\"Blue-Gray\")\n",
        "    print(df[\"Blue-Gray\"].isnull().sum())\n",
        "    print(\"Black\")\n",
        "    print(df[\"Black\"].isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnx7LxkM1Cma"
      },
      "source": [
        "# Tratando valores NaN nas colunas de cores\n",
        "col = [\"White\", \"Red\", \"Light-Brown\", \"Dark-Brown\", \"Blue-Gray\", \"Black\"]\n",
        "for c in col: \n",
        "    vet = np.zeros(200)\n",
        "    cont = 0\n",
        "    for el in df[c]:\n",
        "        if el == 'X':\n",
        "            vet[cont] = 1\n",
        "        cont += 1\n",
        "    df[c] = vet.astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxruU9oEKHPV"
      },
      "source": [
        "# Fazendo o LabelEncoder nos atributos string\n",
        "le = LabelEncoder()\n",
        "df['Pigment Network(AT/T)'] = le.fit_transform(df['Pigment Network(AT/T)'])\n",
        "df['Dots/Globules(A/AT/T)'] = le.fit_transform(df['Dots/Globules(A/AT/T)'])\n",
        "df['Streaks\\n(A/P)'] = le.fit_transform(df['Streaks\\n(A/P)'])\n",
        "df['Regression Areas(A/P)'] = le.fit_transform(df['Regression Areas(A/P)'])\n",
        "df['Blue-Whitish Veil(A/P)'] = le.fit_transform(df['Blue-Whitish Veil(A/P)'])\n",
        "\n",
        "# Fazendo o OneHotEncoder\n",
        "x = df[['Pigment Network(AT/T)','Dots/Globules(A/AT/T)','Streaks\\n(A/P)','Regression Areas(A/P)','Blue-Whitish Veil(A/P)']]\n",
        "y = OneHotEncoder().fit_transform(x).toarray()\n",
        "\n",
        "# Contatenando com a base do onehotenc\n",
        "col = ['Pigment Network(AT/T)_1', 'Pigment Network(AT/T)_2','Dots/Globules(A/AT/T)_1','Dots/Globules(A/AT/T)_2','Dots/Globules(A/AT/T)_3','Streaks\\n(A/P)_1','Streaks\\n(A/P)_2','Regression Areas(A/P)_1','Regression Areas(A/P)_2','Blue-Whitish Veil(A/P)_1','Blue-Whitish Veil(A/P)_2']\n",
        "df2 = pd.DataFrame(y, columns=col).astype(int)\n",
        "\n",
        "df_ = pd.concat([df, df2], axis=1)\n",
        "\n",
        "# Apagando colunas antigas\n",
        "df_ = df_.drop(columns=['Pigment Network(AT/T)','Dots/Globules(A/AT/T)','Streaks\\n(A/P)','Regression Areas(A/P)','Blue-Whitish Veil(A/P)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOQPvsxiQrNb"
      },
      "source": [
        "# Dividindo em atributos e labels\n",
        "com_one_hot = 1\n",
        "if com_one_hot == 1:\n",
        "    df_atributos = df_.iloc[:, 3:21].values\n",
        "    df_label = df_.iloc[:, 2].values\n",
        "else:\n",
        "    df_atributos = df.iloc[:, 3:15].values\n",
        "    df_label = df.iloc[:, 2].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtrXnSLlRzJB"
      },
      "source": [
        "# Padronização # TA ERRADO, tem q ser feito depois\n",
        "com_padronizacao = 0\n",
        "if com_padronizacao == 1:\n",
        "    scl = StandardScaler()\n",
        "    df_atributos = scl.fit_transform(df_atributos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46v42kdQSDUM"
      },
      "source": [
        "# Dividindo em treino e teste\n",
        "df_atributos_train, df_atributos_test, df_label_train, df_label_test = train_test_split(df_atributos, df_label, test_size=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scqSMOVHSZT9",
        "outputId": "aeffd7e6-83c7-4263-8ed4-6107c3657583",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#classificador = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
        "#classificador = DecisionTreeClassifier(criterion='entropy')\n",
        "#classificador = GaussianNB()\n",
        "#classificador = RandomForestClassifier(n_estimators = 5,criterion='entropy')\n",
        "#classificador = LogisticRegression()\n",
        "classificador = SVC(kernel = 'rbf')\n",
        "\n",
        "classificador.fit(df_atributos_train, df_label_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zkd9hrdJSlJG"
      },
      "source": [
        "previsoes = classificador.predict(df_atributos_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbzeL9bCStHo",
        "outputId": "a1117f99-696d-429c-951e-e6240701e08a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "accuracy_score(df_label_test, previsoes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.85"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eErP-3LjVENI",
        "outputId": "e03259c8-ddc4-4215-9771-900e1957c131",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "confusion_matrix(df_label_test, previsoes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[16,  0,  0],\n",
              "       [ 1, 12,  3],\n",
              "       [ 0,  2,  6]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TMBngC7XGrL",
        "outputId": "3e2d70e0-bd7b-4171-aa59-af96514d8167",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=5, random_state = 0, shuffle=True)\n",
        "\n",
        "ac = np.zeros(5)\n",
        "cont = 0\n",
        "for train_index, test_index in kf.split(df_atributos):\n",
        "    #print(\"\\n\\nTRAIN:\", train_index, \"\\nTEST:\", test_index)\n",
        "    df_atributos_train, df_atributos_test = df_atributos[train_index], df_atributos[test_index]\n",
        "    df_label_train, df_label_test = df_label[train_index], df_label[test_index]\n",
        "    \n",
        "    classificador.fit(df_atributos_train, df_label_train)\n",
        "    previsoes = classificador.predict(df_atributos_test)\n",
        "    \n",
        "    #print(\"Confusion Matrix\")\n",
        "    #print(confusion_matrix(df_label_test, previsoes))\n",
        "    #print()\n",
        "\n",
        "    ac[cont] = accuracy_score(df_label_test, previsoes)\n",
        "    cont += 1\n",
        "\n",
        "print(ac)\n",
        "print(\"Acuracia: %0.2f\" % (ac.mean()))\n",
        "print(\"STD: %0.2f\" % (ac.std()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.875 0.95  0.875 0.9   0.85 ]\n",
            "Acuracia: 0.89\n",
            "STD: 0.03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg4AGdsibqvN",
        "outputId": "dd089bce-fd6f-44d5-82a9-8a37a153e549",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=5, random_state = 0, shuffle=True)\n",
        "\n",
        "ac = np.zeros(5)\n",
        "cont = 0\n",
        "for train_index, test_index in skf.split(df_atributos, df_label):\n",
        "    #print(\"\\n\\nTRAIN:\", train_index, \"\\nTEST:\", test_index)\n",
        "    df_atributos_train, df_atributos_test = df_atributos[train_index], df_atributos[test_index]\n",
        "    df_label_train, df_label_test = df_label[train_index], df_label[test_index]\n",
        "    \n",
        "    classificador.fit(df_atributos_train, df_label_train)\n",
        "    previsoes = classificador.predict(df_atributos_test)\n",
        "    \n",
        "    #print(\"Confusion Matrix\")\n",
        "    #print(confusion_matrix(df_label_test, previsoes))\n",
        "    #print()\n",
        "\n",
        "    ac[cont] = accuracy_score(df_label_test, previsoes)\n",
        "    cont += 1\n",
        "\n",
        "print(ac)\n",
        "print(\"Acuracia: %0.2f\" % (ac.mean()))\n",
        "print(\"STD: %0.2f\" % (ac.std()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.85  0.925 0.825 0.9   0.85 ]\n",
            "Acuracia: 0.87\n",
            "STD: 0.04\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}